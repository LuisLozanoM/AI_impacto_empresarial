{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30df511f",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è IMPORTANT: Use the New Tutorial Instead!\n",
    "\n",
    "## üéØ A complete working LangGraph tutorial has been created!\n",
    "\n",
    "**New File**: `langgraph_tutorial_local.ipynb`\n",
    "\n",
    "This new notebook contains:\n",
    "- ‚úÖ Complete LangGraph quickstart replication\n",
    "- ‚úÖ Works with **Ollama + Llama 3.2 3B** (local model)\n",
    "- ‚úÖ Full tool calling support (no workarounds needed)\n",
    "- ‚úÖ Step-by-step implementation matching the official tutorial\n",
    "- ‚úÖ Multiple test examples\n",
    "\n",
    "## üöÄ Quick Start:\n",
    "\n",
    "1. **The model is currently downloading** (Llama 3.2 3B - ~2GB)\n",
    "2. **Open**: `langgraph_tutorial_local.ipynb`\n",
    "3. **Wait** for the model download to complete\n",
    "4. **Run** all cells in the new notebook\n",
    "\n",
    "---\n",
    "\n",
    "**This notebook below** contains experimental code with Qwen2-0.5B which **doesn't support tool calling**. Keep it for reference, but use the new tutorial for the LangGraph quickstart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c5acb",
   "metadata": {},
   "source": [
    "# LangChain Tool Calling with Local Models\n",
    "\n",
    "## Status: ‚ö†Ô∏è Tool Calling Setup\n",
    "\n",
    "**Current Issue:** The Qwen2-0.5B model doesn't support native tool calling (function calling).\n",
    "\n",
    "### ‚úÖ Solution Options:\n",
    "\n",
    "1. **Ollama + Llama 3.2** (Recommended for local use)\n",
    "   - Ollama has been installed\n",
    "   - After restarting VS Code, run in terminal: `ollama pull llama3.2:3b`\n",
    "   - Then use code: `model = init_chat_model(\"ollama:llama3.2:3b\", temperature=0)`\n",
    "   \n",
    "2. **API-based models** (Best tool calling support)\n",
    "   - OpenAI GPT-4: `model = init_chat_model(\"openai:gpt-4\", temperature=0)`\n",
    "   - Anthropic Claude: `model = init_chat_model(\"anthropic:claude-3-5-sonnet\", temperature=0)`\n",
    "\n",
    "3. **Current workaround** (Limited capabilities)\n",
    "   - Using Qwen2-0.5B with manual tool prompting (see cells below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47546ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen2-0.5B model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded! Available tools:\n",
      "- add: Adds `a` and `b`.\n",
      "\n",
      "    Args:\n",
      "        a: First int\n",
      "        b: Second int\n",
      "- multiply: Multiply `a` and `b`.\n",
      "\n",
      "    Args:\n",
      "        a: First int\n",
      "        b: Second int\n",
      "- divide: Divide `a` and `b`.\n",
      "\n",
      "    Args:\n",
      "        a: First int\n",
      "        b: Second int\n",
      "\n",
      "Note: This model doesn't have native tool calling. For better tool support,\n",
      "consider using: OpenAI, Anthropic, or installing Ollama with llama3.2 or mistral models.\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from typing import List, Optional\n",
    "import json\n",
    "\n",
    "# For tool calling, we'll use a simpler approach with Qwen2-0.5B\n",
    "# Note: Small models like Qwen2-0.5B don't have native tool calling,\n",
    "# so we'll create a simple agent pattern instead\n",
    "\n",
    "print(\"Loading Qwen2-0.5B model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply `a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds `a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide `a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "# Create tool descriptions for prompting\n",
    "tool_descriptions = \"\\n\".join([\n",
    "    f\"- {tool.name}: {tool.description}\" for tool in tools\n",
    "])\n",
    "\n",
    "print(\"\\nModel loaded! Available tools:\")\n",
    "print(tool_descriptions)\n",
    "print(\"\\nNote: This model doesn't have native tool calling. For better tool support,\")\n",
    "print(\"consider using: OpenAI, Anthropic, or installing Ollama with llama3.2 or mistral models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f671e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# OPTION 2: Proper Tool Calling with Ollama + Llama 3.2\n",
    "# ========================================\n",
    "# Run this cell AFTER you've completed these steps:\n",
    "# 1. Restart VS Code (so Ollama is in PATH)\n",
    "# 2. Open terminal and run: ollama pull llama3.2:3b\n",
    "# 3. Then uncomment and run this cell\n",
    "\n",
    "\"\"\"\n",
    "from langchain.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Use Llama 3.2 which supports tool calling\n",
    "model = init_chat_model(\n",
    "    \"ollama:llama3.2:3b\",  # or \"ollama:mistral\" or \"ollama:llama3.1\"\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    '''Multiply `a` and `b`.'''\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    '''Adds `a` and `b`.'''\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    '''Divide `a` and `b`.'''\n",
    "    return a / b\n",
    "\n",
    "# Bind tools - THIS WILL WORK with llama3.2!\n",
    "tools = [add, multiply, divide]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "# Test it\n",
    "response = model_with_tools.invoke(\"What is 25 times 4?\")\n",
    "print(\"Response:\", response)\n",
    "print(\"\\nTool calls:\", response.tool_calls)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191fe209",
   "metadata": {},
   "source": [
    "## üìã Summary\n",
    "\n",
    "### The Problem:\n",
    "- **Qwen2-0.5B doesn't support tool/function calling** natively\n",
    "- The `bind_tools()` method only works with models that have built-in function calling\n",
    "\n",
    "### ‚úÖ Solutions:\n",
    "\n",
    "| Solution | Tool Support | Setup Required | Performance |\n",
    "|----------|-------------|----------------|-------------|\n",
    "| **Qwen2-0.5B (Current)** | ‚ùå No native support | ‚úÖ Ready now | Fast but limited |\n",
    "| **Ollama + Llama 3.2** | ‚úÖ Full support | üîÑ Restart VS Code needed | Good (local) |\n",
    "| **OpenAI/Anthropic** | ‚úÖ Best support | üîë API key needed | Excellent (cloud) |\n",
    "\n",
    "### üöÄ Next Steps to Enable Tool Calling:\n",
    "\n",
    "1. **Restart VS Code** (so Ollama is available in PATH)\n",
    "2. Open a new terminal\n",
    "3. Run: `ollama pull llama3.2:3b`\n",
    "4. Uncomment and run the cell below\n",
    "\n",
    "### Models with Tool Calling Support:\n",
    "- ‚úÖ Llama 3.1/3.2 (via Ollama)\n",
    "- ‚úÖ Mistral (via Ollama)\n",
    "- ‚úÖ GPT-4, GPT-3.5-turbo (OpenAI)\n",
    "- ‚úÖ Claude 3.5, Claude 3 (Anthropic)\n",
    "- ‚ùå Qwen2-0.5B (too small)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
