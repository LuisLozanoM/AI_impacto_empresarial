{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a2c02d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mnist = load_dataset(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6bdeee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+trwt4W1TxfrcOl6VDvlc5eRvuRL3Zj2A/wD1V0Xj74YXvgeKG5S+TU7NmMU00MRUW8oCnY/JAzu455x0FcHVnTrC51XUrbT7OMyXNzKsUaDuxOBXrmveMYfhfpk/gfwsG/tCJ1e/1TOC8pwWVR2GMLnIxg9+aboGsX/jvwD8SLrX7tpXjitrqPaAio6iTGAOMHYo98eteOV0ngC8t7D4gaDd3cyQ28V5G0kjnCqM9SfSvRvFXwjgttb1LxDr/i6xs9Hup3uY5QpeaYMxbaqjAJwe2fpXDeJ/Fdk+nHw34Whez8PKweTzADNeSA/6yRuuPReAPT046iiiiv/Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABIUlEQVR4AWIYBMB4/t/5RjjcYfDuz58/b7FLmj3+++f9yz+WbJjSXDYP/vz9cyrkz98qiCQThAKTMw/IMDAwGPEcZNAF85Eljb0ZGQ+VMr44P4mJESKJACC3bObxrhRlYPj7Gc3Bakv/vrwQAlH7989SCAMK2Df9+eAuDLKTgYHh75/DUGEIsPzzxx7CwiJ57O8+uBzD/79HwByoV3wM/m8C88HEv/8XwDQUhP55JgllMrC3/93FA+OAQOif+yAKhNmb/zx0BzHgOPTPRCjbYOmftVAmjAIs7O9DCLPo3d9FEBaCDP3zc5KBbOimh3/vL7dACENYoX/+/Hl6/c+fP0eaIALIpMzxP3/+/vnzEmYzshwDg2TDn79/elVRBQeCBwCscG5E1y1scgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist[\"train\"][\"image\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "171e71f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist[\"train\"][\"label\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e10b4834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from genaibook.core import show_images\n",
    "\n",
    "show_images(mnist[\"train\"][\"image\"][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73a92ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def mnist_to_tensor(samples):\n",
    "    t = transforms.ToTensor()\n",
    "    samples[\"image\"] = [t(img) for img in samples[\"image\"]]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4dc3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = mnist.with_transform(mnist_to_tensor)\n",
    "mnist[\"train\"] = mnist[\"train\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20690ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = mnist[\"train\"][\"image\"][0]\n",
    "x.min(), x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "028ea874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(mnist[\"train\"][\"image\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2efa807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bs = 64\n",
    "train_dataloader = DataLoader(mnist[\"train\"][\"image\"], batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "edd09e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            stride=stride, \n",
    "            padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2cc429fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, latent_dims):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            conv_block(in_channels, 128),\n",
    "            conv_block(128, 256),\n",
    "            conv_block(256, 512),\n",
    "            conv_block(512, 1024),\n",
    "        )\n",
    "        self.linear = nn.Linear(1024, latent_dims)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.linear(x.reshape(bs, -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f306a60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist[\"train\"][\"image\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5910456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 1\n",
    "x = mnist[\"train\"][\"image\"][0][None, :]\n",
    "encoder = Encoder(in_channels, 2).eval()\n",
    "\n",
    "encoded = encoder(x)\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2494dbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0088, 0.0276]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "53e2a316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64, 2]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "encoded = Encoder(in_channels=1, latent_dims=2)(batch)\n",
    "batch.shape, encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5255079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_transpose_block(\n",
    "    in_channels,\n",
    "    out_channels,\n",
    "    kernel_size=3,\n",
    "    stride=2,\n",
    "    padding=1,\n",
    "    output_padding=0,\n",
    "    with_act=True,\n",
    "):\n",
    "    modules = [\n",
    "        nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            output_padding=output_padding,\n",
    "        ),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "    ]\n",
    "    if with_act:\n",
    "        modules.append(nn.BatchNorm2d(out_channels))\n",
    "        modules.append(nn.ReLU())\n",
    "    return nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db390aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_channels, latent_dims):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(latent_dims, 1024 * 4 * 4)\n",
    "        self.t_conv_layers = nn.Sequential(\n",
    "            conv_transpose_block(1024, 512),\n",
    "            conv_transpose_block(512, 256, output_padding=1),\n",
    "            conv_transpose_block(256, out_channels, output_padding=1, with_act=False),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape((bs, 1024, 4, 4))\n",
    "        x = self.t_conv_layers(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d9b36436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_batch = Decoder(x.shape[0], latent_dims=2)(encoded)\n",
    "decoded_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "273fc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, latent_dims):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_channels, latent_dims)\n",
    "        self.decoder = Decoder(in_channels, latent_dims)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7848732",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f747e6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 14, 14]           2,176\n",
      "       BatchNorm2d-2          [-1, 128, 14, 14]             256\n",
      "              ReLU-3          [-1, 128, 14, 14]               0\n",
      "            Conv2d-4            [-1, 256, 7, 7]         524,544\n",
      "       BatchNorm2d-5            [-1, 256, 7, 7]             512\n",
      "              ReLU-6            [-1, 256, 7, 7]               0\n",
      "            Conv2d-7            [-1, 512, 3, 3]       2,097,664\n",
      "       BatchNorm2d-8            [-1, 512, 3, 3]           1,024\n",
      "              ReLU-9            [-1, 512, 3, 3]               0\n",
      "           Conv2d-10           [-1, 1024, 1, 1]       8,389,632\n",
      "      BatchNorm2d-11           [-1, 1024, 1, 1]           2,048\n",
      "             ReLU-12           [-1, 1024, 1, 1]               0\n",
      "           Linear-13                    [-1, 2]           2,050\n",
      "          Encoder-14                    [-1, 2]               0\n",
      "           Linear-15                [-1, 16384]          49,152\n",
      "  ConvTranspose2d-16            [-1, 512, 7, 7]       4,719,104\n",
      "      BatchNorm2d-17            [-1, 512, 7, 7]           1,024\n",
      "      BatchNorm2d-18            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-19            [-1, 512, 7, 7]               0\n",
      "  ConvTranspose2d-20          [-1, 256, 14, 14]       1,179,904\n",
      "      BatchNorm2d-21          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-22          [-1, 256, 14, 14]             512\n",
      "             ReLU-23          [-1, 256, 14, 14]               0\n",
      "  ConvTranspose2d-24            [-1, 1, 28, 28]           2,305\n",
      "      BatchNorm2d-25            [-1, 1, 28, 28]               2\n",
      "          Sigmoid-26            [-1, 1, 28, 28]               0\n",
      "          Decoder-27            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 16,973,445\n",
      "Trainable params: 16,973,445\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.44\n",
      "Params size (MB): 64.75\n",
      "Estimated Total Size (MB): 68.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "\n",
    "torchsummary.summary(model, input_size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ef991e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from genaibook.core import get_device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc4689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing training function and step here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d0425a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs=10, lr=1e-4):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, eps=1e-5)\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    for _ in (progress := trange(num_epochs, desc=\"Training\")):\n",
    "        for _, batch in (\n",
    "            inner := tqdm(\n",
    "                enumerate(train_dataloader), total=len(train_dataloader)\n",
    "            )\n",
    "        ):\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            preds = model(batch)\n",
    "            \n",
    "            loss = F.mse_loss(preds, batch)\n",
    "            \n",
    "            inner.set_postfix(loss=f\"{loss.cpu().item():.3f}\")\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        progress.set_postfix(loss=f\"{loss.cpu().item():.3f}\", lr=f\"{lr:.0e}\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "56cb32da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Encoder(\n",
       "    (conv_layers): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(1, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (linear): Linear(in_features=2, out_features=16384, bias=True)\n",
       "    (t_conv_layers): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ConvTranspose2d(256, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_model = AutoEncoder(in_channels=1, latent_dims=2)\n",
    "ae_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6040ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361e3af1f2aa4a83adcee53e6c8e0e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2beee035368f4aa181671e465df54a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = train(ae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22928b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve (two latent dimensions)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61be613",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_bs = 16\n",
    "eval_dataloader = DataLoader(mnist[\"test\"][\"image\"], batch_size=eval_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.eval()\n",
    "with torch.inference_mode():\n",
    "    eval_batch = next(iter(eval_dataloader))\n",
    "    predicted = ae_model(eval_batch.to(device)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_vs_preds = torch.cat((eval_batch, predicted))\n",
    "show_images(batch_vs_preds, imsize=1, nrow=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c08b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_labels_dataloader = DataLoader(mnist[\"test\"], batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": [],\n",
    "        \"y\": [],\n",
    "        \"label\": [],\n",
    "    }\n",
    ")\n",
    "\n",
    "for batch in tqdm(\n",
    "    iter(images_labels_dataloader), total=len(images_labels_dataloader)\n",
    "):\n",
    "    encoded = ae_model.encode(batch[\"image\"].to(device)).cpu()\n",
    "    new_items = {\n",
    "        \"x\": [t.tiem() for t in encoded[:, 0]],\n",
    "        \"y\": [t.item() for t in encoded[:, 1]],\n",
    "        \"label\": batch[\"label\"],\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame(new_items)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for label in range(10):\n",
    "    points = df[df[\"label\"] == label]\n",
    "    plt.scatter(points[\"x\"], points[\"y\"], label=str(label), marker=\".\")\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28ba71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8ef79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
