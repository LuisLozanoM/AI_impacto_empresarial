# AI Impacto Empresarial

AI and ML experiments with transformers, diffusion models, and text generation using Python 3.11.

## Overview

This repository contains experiments with various AI and machine learning models, focusing on:

- ü§ñ **Transformers**: Working with Hugging Face transformers library
- üìù **Text Generation**: Exploring different text generation techniques
- üéØ **Model Analysis**: Understanding model behavior and token probabilities
- üî¨ **Sampling Methods**: Comparing different sampling strategies

## Contents

- `text_models.ipynb`: Main notebook with transformer experiments including:
  - Tokenization analysis
  - Model loading and inference
  - Probability analysis of predictions
  - Text generation with different strategies (greedy, beam search, sampling)
  - Temperature and top-k sampling experiments

## Environment Setup

This project uses a custom Python 3.11 environment with the following key packages:

- **transformers** 4.56.0 - Hugging Face transformers library
- **torch** 2.8.0+cpu - PyTorch for deep learning
- **numpy** 2.3.2 - Numerical computing
- **pandas** 2.3.2 - Data manipulation
- **matplotlib** 3.10.6 - Data visualization
- **diffusers** 0.35.1 - Diffusion models
- **ollama** 0.5.3 - Local LLM integration
- **gradio** 5.44.1 - ML demos and interfaces
- **streamlit** 1.49.1 - Data apps
- **wandb** 0.21.3 - Experiment tracking

### Installation

```bash
# Create virtual environment
python -m venv ai_ml_env

# Activate environment (Windows)
.\ai_ml_env\Scripts\Activate.ps1

# Install packages
pip install transformers torch diffusers ollama jupyter pandas numpy matplotlib seaborn scikit-learn plotly gradio streamlit wandb tensorboard

# Register kernel for Jupyter
python -m ipykernel install --user --name ai_ml_env --display-name "AI/ML Environment (Python 3.11)"
```

## Usage

1. Activate the environment
2. Launch Jupyter Lab or open the notebook in VS Code
3. Select the "AI/ML Environment (Python 3.11)" kernel
4. Run the cells to explore transformer models and text generation

## Models Used

- **Qwen/Qwen2-0.5B**: A small but capable language model for experimentation

## Features Demonstrated

- **Tokenization**: Understanding how text is converted to tokens
- **Model Inference**: Getting predictions from transformer models
- **Probability Analysis**: Examining model confidence and token probabilities
- **Generation Strategies**: 
  - Greedy decoding
  - Beam search
  - Sampling with temperature control
  - Top-k sampling

## Contributing

Feel free to contribute by:
- Adding new model experiments
- Exploring different generation techniques
- Improving documentation
- Adding visualization examples

## License

This project is open source and available under the MIT License.